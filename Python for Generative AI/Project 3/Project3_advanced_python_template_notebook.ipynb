{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OmuaFwVYu5N2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project-3: Finsights Grey - RAG for Effective Information Retrieval\n"
      ],
      "metadata": {
        "id": "N-SSaZ34Ejlu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Business Use Case"
      ],
      "metadata": {
        "id": "_aswaepKTclI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem Statement:**\n",
        "\n",
        "Finsights Grey Inc. is an innovative financial technology firm that specializes in providing advanced analytics and insights for investment management and financial planning. The company handles an extensive collection of 10-K reports from various industry players, which contain detailed information about financial performance, risk factors, market trends, and strategic initiatives. Despite the richness of these documents, Finsights Grey's financial analysts struggle with extracting actionable insights efficiently in a short span due to the manual and labor-intensive nature of the analysis. Going through the document to find the exact information needed at the moment takes too long. This bottleneck hampers the company's ability to deliver timely and accurate recommendations to its clients. To overcome these challenges, Finsights Grey Inc. aims to implement a Retrieval-Augmented Generation (RAG) model to automate the extraction, summarization, and analysis of information from the 10-K reports, thereby enhancing the accuracy and speed of their investment insights.\n",
        "\n",
        "**Objective:**\n",
        "\n",
        "As a Gen AI Data Scientist hired by Finsights Grey Inc., the objective is to develop an advanced RAG-based system to streamline the extraction and analysis of key information from 10-K reports. You are asked to deploy a Gradio app on HuggingFace spaces that can RAG 10-k reports and answer the questions of financial analysts swiftly.\n",
        "\n",
        "The project will involve testing the RAG system on a current business problem. The Financial analysts are asked to research major cloud and AI platforms such as Amazon AWS, Google Cloud, Microsoft Azure, Meta AI, and IBM Watson to determine the most effective platform for this application. The primary goals include improving the efficiency of data extraction. Once the project is deployed, the system will be tested by a financial analyst with the following questions. Accurate text retrieval for these questions will imply the project's success.\n",
        "\n",
        "**Questions:**\n",
        "\n",
        "1. Has the company made any significant acquisitions in the AI space, and how are these acquisitions being integrated into the company's strategy?\n",
        "\n",
        "2. How much capital has been allocated towards AI research and development?\n",
        "\n",
        "3. What initiatives has the company implemented to address ethical concerns surrounding AI, such as fairness, accountability, and privacy?\n",
        "\n",
        "4. How does the company plan to differentiate itself in the AI space relative to competitors?\n",
        "\n",
        "Each Question must be asked for each of the five companies on the HuggingFace spaces.\n",
        "\n",
        "\n",
        "**By successfully developing this project, we aim to:**\n",
        "\n",
        "Improve the productivity of financial analysts by providing a competent tool.\n",
        "\n",
        "Provide timely insights to improve client recommendations.\n",
        "\n",
        "Strengthen FinTech Insights Inc.â€™s competitive edge by delivering more reliable and faster insights to clients.\n"
      ],
      "metadata": {
        "id": "1_Nst0jMYzGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Connect to a T4 GPU Instance to create the Vector Database.**"
      ],
      "metadata": {
        "id": "QaB-Mqiprrb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "UgAFFFruPmpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary libraries\n"
      ],
      "metadata": {
        "id": "yC_5gAl0PoPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKMjqQpFl4kd"
      },
      "outputs": [],
      "source": [
        "# Import the necessary Libraries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Impementing RAG"
      ],
      "metadata": {
        "id": "1OhFS7PMZFUA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Data"
      ],
      "metadata": {
        "id": "WT9dh1VXTfaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start by loading the dataset."
      ],
      "metadata": {
        "id": "DiQVSdzoT4hf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload Dataset-10k.zip and unzip it dataset folder using -d option\n",
        "!unzip Dataset-10k.zip -d dataset"
      ],
      "metadata": {
        "id": "w3GmULkltJ4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DB Creation"
      ],
      "metadata": {
        "id": "yt8gTrjLNqgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking"
      ],
      "metadata": {
        "id": "xlmmenddNiYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide pdf_folder_location\n",
        "pdf_folder_location = \"\""
      ],
      "metadata": {
        "id": "Um45nt-AUDJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the directory to pdf_loader\n",
        "pdf_loader ="
      ],
      "metadata": {
        "id": "LZPJAjBpsEq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create text_splitter using recursive splitter\n",
        "text_splitter ="
      ],
      "metadata": {
        "id": "rWpisKbHqbB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create chunks\n",
        "report_chunks ="
      ],
      "metadata": {
        "id": "R0tRVdBNrCuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the total number of chunks\n"
      ],
      "metadata": {
        "id": "lw7FoLzfrKOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the first object in report_chunks and print it"
      ],
      "metadata": {
        "id": "oVfipQe-CL0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Database Creation"
      ],
      "metadata": {
        "id": "k_N1t3Fdbn1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a Colelction Name\n",
        "collection_name = ''"
      ],
      "metadata": {
        "id": "WiN4bZqGvVdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate the embedding model 'thenlper/gte-large'"
      ],
      "metadata": {
        "id": "nwusGdTRxhhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the vector Database\n",
        "vectorstore ="
      ],
      "metadata": {
        "id": "VskCNDYLufyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Persist the DB"
      ],
      "metadata": {
        "id": "9Y6xEorcdO2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount the Google Drive"
      ],
      "metadata": {
        "id": "WKirzhpjtc0H",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Copy the persisted database to your drive"
      ],
      "metadata": {
        "id": "i5LcRZwRZfAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieve DB from GDrive"
      ],
      "metadata": {
        "id": "63vOMlzZr3_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Set up CPU Instance**"
      ],
      "metadata": {
        "id": "SbAoNV-p_qzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required packages\n"
      ],
      "metadata": {
        "id": "2hjde8i9_vru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqCsRMmk_vsB"
      },
      "outputs": [],
      "source": [
        "# Import the necessary Libraries\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up Anyscale Credentials"
      ],
      "metadata": {
        "id": "EUkvNfpk_vsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get anyscale api key"
      ],
      "metadata": {
        "id": "lc758roT_vsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise the client"
      ],
      "metadata": {
        "id": "In-6IKIu_vsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Provide the model name"
      ],
      "metadata": {
        "id": "8mpSn4JH_vsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Google Drive"
      ],
      "metadata": {
        "id": "wM1yEibVATdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount the Google Drive"
      ],
      "metadata": {
        "id": "luDxXO3MAYDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Vector DB from Google Drive"
      ],
      "metadata": {
        "id": "G972qLW-uI3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise the embedding model"
      ],
      "metadata": {
        "id": "GVwgNoHguTMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the persisted DB\n",
        "persisted_vectordb_location = ''"
      ],
      "metadata": {
        "id": "cOuFyoLzaQ_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a Colelction Name\n",
        "collection_name = ''"
      ],
      "metadata": {
        "id": "3Slhfsp3AFLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the persisted DB\n"
      ],
      "metadata": {
        "id": "o3VQmzZnuLzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test your DB"
      ],
      "metadata": {
        "id": "cOZOClNCtRNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"How is the company integrating AI across their various business units, and what specific examples are provided in the reports of AI enhancing operational efficiencies or customer experiences?\""
      ],
      "metadata": {
        "id": "BORXQBpDD9pj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform similarity search on the user_question\n",
        "# You must add an extra parameter to the similarity search  function so that you can filter the response based on the 'source'  in the metadata of the doc\n",
        "# The filter can be added as a parameter to the similarity search function\n",
        "# This will allow you to retrieve chunks from a particular document\n",
        "# Use the same format to filter your response based on the company.\n",
        "docs = reports_db.similarity_search(user_question, k=5, filter = {\"source\":\"dataset/google-10-k-2023.pdf\"}) # Note the format to add a filter. You must apply the same in your app.py file that you will upload on huggingface spaces"
      ],
      "metadata": {
        "id": "5v9d4I4WcmSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the retrieved docs, their source and the page number\n",
        "# (page number can be accessed using doc.metadata['page'] )\n"
      ],
      "metadata": {
        "id": "z1qh4cktdg7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RAG Q&A"
      ],
      "metadata": {
        "id": "pv9oH4ukXLSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Design"
      ],
      "metadata": {
        "id": "s343-PgW6P-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a system message for the LLM\n",
        "qna_system_message = \"\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LR4dzgL96U0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a message template\n",
        "qna_user_message_template = \"\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bDexqi8c6Xmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Composing the response"
      ],
      "metadata": {
        "id": "G_ekBjVM60P1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a variable company to store the source of the context so that you can filter the similarity search\n",
        "company = \"dataset/aws-10-k-2023.pdf\""
      ],
      "metadata": {
        "id": "nCaep30PJeDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch relevant documents and create context for query by joining page_content and page number of the retrieved docs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print() # Print the whole context_for_query (after joining all the chunks. It should contain page number of every chunk)"
      ],
      "metadata": {
        "id": "aHXY6BcV676h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Craft the messages to pass to chat.completions.create\n"
      ],
      "metadata": {
        "id": "WknFs_EVgeF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a response from the LLM\n",
        "# Handle errors using try-except\n",
        "# print the content of the response\n"
      ],
      "metadata": {
        "id": "80DjKW0Xg6-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "DcSJf2Z37Nhg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Craft prompts for evaluation"
      ],
      "metadata": {
        "id": "QpD2svFIM7gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick a model that's offers more performace as a rater_model. Most of the time a model with more parameters is more performant.\n",
        "rater_model = \"\""
      ],
      "metadata": {
        "id": "_Mv-9eLKkO5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a prompt for the rater LLM to check the groundedness of the response\n",
        "groundedness_rater_system_message = \"\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2dDxkZdyKSnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a prompt for the rater LLM to check the relevance of the response\n",
        "relevance_rater_system_message = \"\"\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "NIosu2Wk7OVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create user message template such that question, answer and context can be provided through it.\n",
        "user_message_template = \"\"\"\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7boMupgh_Gux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test the Evaluation on One Sample"
      ],
      "metadata": {
        "id": "GdBhlbmlLMJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_input = \"How much is the company investing in research and development, and what are the key areas of focus for innovation?\""
      ],
      "metadata": {
        "id": "GHoSgUumAmzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch relevant documents and create context for query by joining page_content and page number of the retrieved docs\n"
      ],
      "metadata": {
        "id": "6RpPvWtZrb0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the messages for chat.completion.create()\n"
      ],
      "metadata": {
        "id": "IgzkA4TmAtKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a response from the LLM\n",
        "# Handle errors using try-except\n"
      ],
      "metadata": {
        "id": "n-d-5j4HBo0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create messages for groundness LLM\n"
      ],
      "metadata": {
        "id": "8VvvK6U4A6tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the response of the rater LLM on groundedness\n"
      ],
      "metadata": {
        "id": "nDDj3KfLBSCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the response of the rater LLM on relevance\n"
      ],
      "metadata": {
        "id": "6ujo5RUKBlpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the response of the rater LLM on relevance\n"
      ],
      "metadata": {
        "id": "mEpI9Kj-BlpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on multiple-queries"
      ],
      "metadata": {
        "id": "-kcSAZbLL5JS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of queries\n",
        "queries = [ \"What are the companyâ€™s policies and frameworks regarding AI ethics, governance, and responsible AI use as detailed in their 10-K reports?\",\n",
        "           \"What are the primary business segments of the company, and how does each segment contribute to the overall revenue and profitability?\",\n",
        "            \"What are the key risk factors identified in the 10-K report that could potentially impact the companyâ€™s business operations and financial performance?\"\n",
        "\n",
        "]\n",
        "# Create a DataFrame to store the results\n",
        "df = pd.DataFrame(columns=['query', 'response', 'context', 'groundedness_evaluation', 'relevance_evaluation'])\n",
        "\n",
        "# run a loop to get answer for every query and every company and then rate them on groundedness and relevance\n",
        "# store the query, response, context,groundedness_evaluation, relevance_evaluation in a dataframe\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k-JtmXvE_4PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your Dataframe should have 15 rows - 3 queries for each of 5 companies - 3*5 = 15\n",
        "# Show the top 10 rows of the dataframe\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "RDtcGJMTsZ1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might experience some hallucination in LLM's response. Try to change your prompt to mitigate this. Selecting a good model will also help mitigating hallucination, increase groundedness and relevance."
      ],
      "metadata": {
        "id": "XmH3MYjoMAH5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Interface"
      ],
      "metadata": {
        "id": "AZ0hiDYNR-Hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "\n",
        "## Setup\n",
        "# Import the necessary Libraries\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create Client\n",
        "\n",
        "\n",
        "# Define the embedding model and the vectorstore\n",
        "\n",
        "# Load the persisted vectorDB\n",
        "\n",
        "\n",
        "# Prepare the logging functionality\n",
        "\n",
        "log_file = Path(\"logs/\") / f\"data_{uuid.uuid4()}.json\"\n",
        "log_folder = log_file.parent\n",
        "\n",
        "scheduler = CommitScheduler(\n",
        "    repo_id=\"---------\",\n",
        "    repo_type=\"dataset\",\n",
        "    folder_path=log_folder,\n",
        "    path_in_repo=\"data\",\n",
        "    every=2\n",
        ")\n",
        "\n",
        "# Define the Q&A system message\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the user message template\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define the predict function that runs when 'Submit' is clicked or when a API request is made\n",
        "def predict(user_input,company):\n",
        "\n",
        "    filter = \"dataset/\"+company+\"-10-k-2023.pdf\"\n",
        "    relevant_document_chunks = vectorstore_persisted.similarity_search(user_input, k=5, filter={\"source\":filter})\n",
        "\n",
        "    # Create context_for_query\n",
        "\n",
        "\n",
        "    # Create messages\n",
        "\n",
        "\n",
        "    # Get response from the LLM\n",
        "\n",
        "\n",
        "    # While the prediction is made, log both the inputs and outputs to a local log file\n",
        "    # While writing to the log file, ensure that the commit scheduler is locked to avoid parallel\n",
        "    # access\n",
        "\n",
        "    with scheduler.lock:\n",
        "        with log_file.open(\"a\") as f:\n",
        "            f.write(json.dumps(\n",
        "                {\n",
        "                    'user_input': user_input,\n",
        "                    'retrieved_context': context_for_query,\n",
        "                    'model_response': prediction\n",
        "                }\n",
        "            ))\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# Set-up the Gradio UI\n",
        "# Add text box and radio button to the interface\n",
        "# The radio button is used to select the company 10k report in which the context needs to be retrieved.\n",
        "\n",
        "textbox = gr.Textbox()\n",
        "company = gr.Radio()\n",
        "\n",
        "# Create the interface\n",
        "# For the inputs parameter of Interface provide [textbox,company]\n",
        "\n",
        "\n",
        "demo.queue()\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "vHvL5tOCSCyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paste your gradio app link and logs link\n",
        "\n",
        "*   app link here\n",
        "\n",
        "*   logs_dataset link here\n",
        "\n",
        "Note: Make sure your Hugging Face space repository and the logs_dataset are set to public. If it's private, the evaluator won't be able to access the app you've built, which could result in losing marks."
      ],
      "metadata": {
        "id": "-T4NaXQaiCSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert ipynb to HTML"
      ],
      "metadata": {
        "id": "2agZVT6-inHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions:\n",
        "1. Go to File\n",
        "2. Download these current working Notebook in to ipynb format\n",
        "3. Now, run the below code, select the notebook from local where you downloaded the file\n",
        "4. Wait for few sec, your notebook will automatically converted in to html format and save in your local pc\n"
      ],
      "metadata": {
        "id": "xuPA2F1dikeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title HTML Convert\n",
        "# Upload ipynb\n",
        "from google.colab import files\n",
        "f = files.upload()\n",
        "\n",
        "# Convert ipynb to html\n",
        "import subprocess\n",
        "file0 = list(f.keys())[0]\n",
        "_ = subprocess.run([\"pip\", \"install\", \"nbconvert\"])\n",
        "_ = subprocess.run([\"jupyter\", \"nbconvert\", file0, \"--to\", \"html\"])\n",
        "\n",
        "# download the html\n",
        "files.download(file0[:-5]+\"html\")\n"
      ],
      "metadata": {
        "id": "p7-uT_rmiqYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Power Ahead!"
      ],
      "metadata": {
        "id": "--d1v03IiuwQ"
      }
    }
  ]
}